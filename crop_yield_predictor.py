#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Crop Yield Predictor - Classe Profissional para PrevisÃ£o de Rendimento de Colheita
================================================================================

Esta classe implementa um preditor robusto de rendimento de colheita seguindo
padrÃµes de engenharia de software e melhores prÃ¡ticas de Machine Learning.

Autor: AnÃ¡lise Profissional de Dados
VersÃ£o: 2.0 - VersÃ£o Profissional
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import logging
from datetime import datetime
from pathlib import Path
import warnings
from typing import Tuple, Dict, List, Optional, Union

# Scikit-learn imports
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')

# ConfiguraÃ§Ã£o de logging profissional
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crop_yield_predictor.log'),
        logging.StreamHandler()
    ]
)

class CropYieldPredictor:
    """
    Preditor profissional de rendimento de colheita com validaÃ§Ã£o cruzada,
    anÃ¡lise de resÃ­duos e capacidade de salvar/carregar modelos.
    
    Attributes:
        model: Modelo de Machine Learning treinado
        scaler: Normalizador de features (opcional)
        feature_columns: Lista das colunas de features
        model_metrics: MÃ©tricas de performance do modelo
    """
    
    def __init__(self, model_type: str = 'linear', normalize: bool = False, model_path: Optional[str] = None):
        """
        Inicializa o preditor de rendimento de colheita.
        
        Args:
            model_type: Tipo do modelo ('linear', 'random_forest', 'gradient_boosting')
            normalize: Se deve normalizar as features
            model_path: Caminho para carregar modelo prÃ©-treinado
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.normalize = normalize
        self.scaler = StandardScaler() if normalize else None
        
        # Definindo colunas de features padrÃ£o
        self.feature_columns = [
            'rainfall_mm', 'soil_quality_index', 'farm_size_hectares', 
            'sunlight_hours', 'fertilizer_kg'
        ]
        
        # Inicializando mÃ©tricas
        self.model_metrics = {}
        self.training_history = []
        
        if model_path:
            self.load_model(model_path)
            self.logger.info(f"Modelo carregado de: {model_path}")
        else:
            self.model = self._create_model(model_type)
            self.logger.info(f"Novo modelo criado: {model_type}")
    
    def _create_model(self, model_type: str):
        """Cria modelo baseado no tipo especificado."""
        models = {
            'linear': LinearRegression(),
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
        }
        
        if model_type not in models:
            raise ValueError(f"Tipo de modelo nÃ£o suportado: {model_type}. OpÃ§Ãµes: {list(models.keys())}")
        
        return models[model_type]
    
    def load_data(self, filepath: Union[str, Path]) -> pd.DataFrame:
        """
        Carrega dados do arquivo CSV.
        
        Args:
            filepath: Caminho para o arquivo CSV
            
        Returns:
            DataFrame com os dados carregados
        """
        try:
            df = pd.read_csv(filepath)
            self.logger.info(f"Dados carregados: {df.shape[0]} linhas x {df.shape[1]} colunas")
            
            # ValidaÃ§Ã£o bÃ¡sica dos dados
            missing_cols = [col for col in self.feature_columns + ['crop_yield'] if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Colunas faltando no dataset: {missing_cols}")
            
            return df
            
        except Exception as e:
            self.logger.error(f"Erro ao carregar dados: {str(e)}")
            raise
    
    def add_noise_simulation(self, X: pd.DataFrame, y: pd.Series, 
                           noise_level: float = 0.1) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Adiciona ruÃ­do aos dados para testar robustez do modelo.
        
        Args:
            X: Features
            y: Target
            noise_level: NÃ­vel de ruÃ­do (proporÃ§Ã£o do desvio padrÃ£o)
            
        Returns:
            Dados com ruÃ­do adicionado
        """
        self.logger.info(f"Adicionando ruÃ­do com nÃ­vel: {noise_level}")
        
        X_noisy = X.copy()
        y_noisy = y.copy()
        
        # Adicionando ruÃ­do Ã s features numÃ©ricas
        for col in X.columns:
            if X[col].dtype in ['int64', 'float64']:
                noise = np.random.normal(0, X[col].std() * noise_level, len(X))
                X_noisy[col] = X[col] + noise
        
        # Adicionando ruÃ­do ao target
        target_noise = np.random.normal(0, y.std() * noise_level, len(y))
        y_noisy = y + target_noise
        
        return X_noisy, y_noisy
    
    def cross_validation_analysis(self, X: pd.DataFrame, y: pd.Series, 
                                cv_folds: int = 5) -> Dict[str, float]:
        """
        Realiza validaÃ§Ã£o cruzada robusta do modelo.
        
        Args:
            X: Features
            y: Target
            cv_folds: NÃºmero de folds para validaÃ§Ã£o cruzada
            
        Returns:
            DicionÃ¡rio com mÃ©tricas de validaÃ§Ã£o cruzada
        """
        self.logger.info(f"Iniciando validaÃ§Ã£o cruzada com {cv_folds} folds")
        
        # Preparando dados
        if self.normalize and self.scaler:
            X_scaled = pd.DataFrame(
                self.scaler.fit_transform(X), 
                columns=X.columns, 
                index=X.index
            )
        else:
            X_scaled = X
        
        # ValidaÃ§Ã£o cruzada
        kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
        
        # RÂ² scores
        r2_scores = cross_val_score(self.model, X_scaled, y, cv=kfold, scoring='r2')
        
        # RMSE scores (negativo, entÃ£o invertemos)
        rmse_scores = np.sqrt(-cross_val_score(self.model, X_scaled, y, cv=kfold, scoring='neg_mean_squared_error'))
        
        # MAE scores (negativo, entÃ£o invertemos)
        mae_scores = -cross_val_score(self.model, X_scaled, y, cv=kfold, scoring='neg_mean_absolute_error')
        
        cv_results = {
            'r2_mean': r2_scores.mean(),
            'r2_std': r2_scores.std(),
            'r2_scores': r2_scores,
            'rmse_mean': rmse_scores.mean(),
            'rmse_std': rmse_scores.std(),
            'rmse_scores': rmse_scores,
            'mae_mean': mae_scores.mean(),
            'mae_std': mae_scores.std(),
            'mae_scores': mae_scores
        }
        
        self.logger.info(f"ValidaÃ§Ã£o cruzada concluÃ­da:")
        self.logger.info(f"  RÂ² mÃ©dio: {cv_results['r2_mean']:.4f} (Â±{cv_results['r2_std']:.4f})")
        self.logger.info(f"  RMSE mÃ©dio: {cv_results['rmse_mean']:.4f} (Â±{cv_results['rmse_std']:.4f})")
        
        return cv_results
    
    def residual_analysis(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """
        Realiza anÃ¡lise de resÃ­duos para validar assumiÃ§Ãµes do modelo.
        
        Args:
            X: Features
            y: Target
            
        Returns:
            DicionÃ¡rio com anÃ¡lise de resÃ­duos
        """
        self.logger.info("Iniciando anÃ¡lise de resÃ­duos")
        
        # Preparando dados
        if self.normalize and self.scaler:
            X_scaled = pd.DataFrame(
                self.scaler.transform(X), 
                columns=X.columns, 
                index=X.index
            )
        else:
            X_scaled = X
        
        # Fazendo previsÃµes
        y_pred = self.model.predict(X_scaled)
        residuals = y - y_pred
        
        # AnÃ¡lise estatÃ­stica dos resÃ­duos
        residual_stats = {
            'mean': residuals.mean(),
            'std': residuals.std(),
            'min': residuals.min(),
            'max': residuals.max(),
            'skewness': residuals.skew() if hasattr(residuals, 'skew') else np.nan,
            'residuals': residuals,
            'predictions': y_pred
        }
        
        self.logger.info(f"AnÃ¡lise de resÃ­duos:")
        self.logger.info(f"  MÃ©dia dos resÃ­duos: {residual_stats['mean']:.4f}")
        self.logger.info(f"  Desvio padrÃ£o: {residual_stats['std']:.4f}")
        
        return residual_stats
    
    def train(self, X: pd.DataFrame, y: pd.Series, 
              test_size: float = 0.2, 
              with_cross_validation: bool = True,
              with_noise_test: bool = True) -> Dict:
        """
        Treina o modelo com anÃ¡lises robustas de performance.
        
        Args:
            X: Features
            y: Target
            test_size: ProporÃ§Ã£o para dados de teste
            with_cross_validation: Se deve fazer validaÃ§Ã£o cruzada
            with_noise_test: Se deve testar com dados com ruÃ­do
            
        Returns:
            DicionÃ¡rio com mÃ©tricas de treinamento
        """
        self.logger.info("Iniciando treinamento robusto do modelo")
        
        # Dividindo dados
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
        
        # Normalizando se necessÃ¡rio
        if self.normalize and self.scaler:
            X_train_scaled = pd.DataFrame(
                self.scaler.fit_transform(X_train), 
                columns=X_train.columns, 
                index=X_train.index
            )
            X_test_scaled = pd.DataFrame(
                self.scaler.transform(X_test), 
                columns=X_test.columns, 
                index=X_test.index
            )
        else:
            X_train_scaled, X_test_scaled = X_train, X_test
        
        # Treinando modelo
        self.model.fit(X_train_scaled, y_train)
        
        # MÃ©tricas bÃ¡sicas
        y_train_pred = self.model.predict(X_train_scaled)
        y_test_pred = self.model.predict(X_test_scaled)
        
        basic_metrics = {
            'train_r2': r2_score(y_train, y_train_pred),
            'test_r2': r2_score(y_test, y_test_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),
            'train_mae': mean_absolute_error(y_train, y_train_pred),
            'test_mae': mean_absolute_error(y_test, y_test_pred)
        }
        
        # AnÃ¡lises robustas opcionais
        training_results = {'basic_metrics': basic_metrics}
        
        if with_cross_validation:
            cv_results = self.cross_validation_analysis(X_train, y_train)
            training_results['cross_validation'] = cv_results
        
        if with_noise_test:
            # Testando com ruÃ­do
            noise_levels = [0.05, 0.1, 0.2]
            noise_results = {}
            
            for noise in noise_levels:
                X_noisy, y_noisy = self.add_noise_simulation(X_train, y_train, noise)
                
                # Retreinando com dados com ruÃ­do
                temp_model = LinearRegression()  # Modelo temporÃ¡rio para teste
                
                if self.normalize:
                    X_noisy_scaled = pd.DataFrame(
                        self.scaler.fit_transform(X_noisy), 
                        columns=X_noisy.columns, 
                        index=X_noisy.index
                    )
                else:
                    X_noisy_scaled = X_noisy
                
                temp_model.fit(X_noisy_scaled, y_noisy)
                y_test_pred_noisy = temp_model.predict(X_test_scaled)
                
                noise_results[f'noise_{noise}'] = {
                    'r2': r2_score(y_test, y_test_pred_noisy),
                    'rmse': np.sqrt(mean_squared_error(y_test, y_test_pred_noisy))
                }
            
            training_results['noise_robustness'] = noise_results
        
        # AnÃ¡lise de resÃ­duos
        residual_analysis = self.residual_analysis(X_test, y_test)
        training_results['residual_analysis'] = residual_analysis
        
        # Salvando mÃ©tricas
        self.model_metrics = training_results
        
        # HistÃ³rico de treinamento
        training_record = {
            'timestamp': datetime.now().isoformat(),
            'metrics': basic_metrics,
            'data_shape': X.shape
        }
        self.training_history.append(training_record)
        
        self.logger.info("Treinamento concluÃ­do com sucesso")
        self.logger.info(f"Performance final - RÂ²: {basic_metrics['test_r2']:.4f}, RMSE: {basic_metrics['test_rmse']:.4f}")
        
        return training_results
    
    def predict(self, data: Union[pd.DataFrame, np.ndarray, List]) -> np.ndarray:
        """
        Faz previsÃµes usando o modelo treinado.
        
        Args:
            data: Dados para previsÃ£o (DataFrame, array ou lista)
            
        Returns:
            Array com previsÃµes
        """
        if self.model is None:
            raise ValueError("Modelo nÃ£o foi treinado. Execute train() primeiro.")
        
        # Convertendo para DataFrame se necessÃ¡rio
        if isinstance(data, list):
            if len(data) != len(self.feature_columns):
                raise ValueError(f"Lista deve ter {len(self.feature_columns)} elementos")
            data = pd.DataFrame([data], columns=self.feature_columns)
        elif isinstance(data, np.ndarray):
            if data.shape[1] != len(self.feature_columns):
                raise ValueError(f"Array deve ter {len(self.feature_columns)} colunas")
            data = pd.DataFrame(data, columns=self.feature_columns)
        
        # Normalizando se necessÃ¡rio
        if self.normalize and self.scaler:
            data_scaled = pd.DataFrame(
                self.scaler.transform(data), 
                columns=data.columns, 
                index=data.index
            )
        else:
            data_scaled = data
        
        predictions = self.model.predict(data_scaled)
        self.logger.info(f"PrevisÃµes realizadas para {len(data)} amostras")
        
        return predictions
    
    def predict_single(self, rainfall: float, soil_quality: int, 
                      farm_size: float, sunlight: float, fertilizer: float) -> float:
        """
        Faz previsÃ£o para uma Ãºnica amostra.
        
        Args:
            rainfall: PrecipitaÃ§Ã£o em mm
            soil_quality: Qualidade do solo (1-10)
            farm_size: Tamanho da fazenda em hectares
            sunlight: Horas de sol
            fertilizer: Quantidade de fertilizante em kg
            
        Returns:
            Rendimento previsto
        """
        data = [rainfall, soil_quality, farm_size, sunlight, fertilizer]
        prediction = self.predict(data)
        return float(prediction[0])
    
    def save_model(self, filepath: Union[str, Path]) -> None:
        """
        Salva o modelo treinado em arquivo.
        
        Args:
            filepath: Caminho para salvar o modelo
        """
        if self.model is None:
            raise ValueError("Nenhum modelo para salvar. Treine o modelo primeiro.")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'model_metrics': self.model_metrics,
            'training_history': self.training_history,
            'normalize': self.normalize
        }
        
        joblib.dump(model_data, filepath)
        self.logger.info(f"Modelo salvo em: {filepath}")
    
    def load_model(self, filepath: Union[str, Path]) -> None:
        """
        Carrega modelo de arquivo.
        
        Args:
            filepath: Caminho do modelo salvo
        """
        try:
            model_data = joblib.load(filepath)
            
            self.model = model_data['model']
            self.scaler = model_data.get('scaler')
            self.feature_columns = model_data.get('feature_columns', self.feature_columns)
            self.model_metrics = model_data.get('model_metrics', {})
            self.training_history = model_data.get('training_history', [])
            self.normalize = model_data.get('normalize', False)
            
            self.logger.info(f"Modelo carregado de: {filepath}")
            
        except Exception as e:
            self.logger.error(f"Erro ao carregar modelo: {str(e)}")
            raise
    
    def plot_residual_analysis(self, figsize: Tuple[int, int] = (15, 10)) -> None:
        """
        Plota anÃ¡lise de resÃ­duos.
        
        Args:
            figsize: Tamanho da figura
        """
        if 'residual_analysis' not in self.model_metrics:
            raise ValueError("AnÃ¡lise de resÃ­duos nÃ£o disponÃ­vel. Treine o modelo primeiro.")
        
        residual_data = self.model_metrics['residual_analysis']
        residuals = residual_data['residuals']
        predictions = residual_data['predictions']
        
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle('ğŸ” ANÃLISE DE RESÃDUOS - VALIDAÃ‡ÃƒO DO MODELO', fontsize=16, fontweight='bold')
        
        # 1. ResÃ­duos vs PrevisÃµes
        axes[0,0].scatter(predictions, residuals, alpha=0.6)
        axes[0,0].axhline(y=0, color='red', linestyle='--')
        axes[0,0].set_xlabel('PrevisÃµes')
        axes[0,0].set_ylabel('ResÃ­duos')
        axes[0,0].set_title('ResÃ­duos vs PrevisÃµes', fontweight='bold')
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. Histograma dos resÃ­duos
        axes[0,1].hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0,1].set_xlabel('ResÃ­duos')
        axes[0,1].set_ylabel('FrequÃªncia')
        axes[0,1].set_title('DistribuiÃ§Ã£o dos ResÃ­duos', fontweight='bold')
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. Q-Q Plot (aproximaÃ§Ã£o)
        sorted_residuals = np.sort(residuals)
        theoretical_quantiles = np.linspace(-3, 3, len(sorted_residuals))
        axes[1,0].scatter(theoretical_quantiles, sorted_residuals, alpha=0.6)
        axes[1,0].plot(theoretical_quantiles, theoretical_quantiles, 'r--')
        axes[1,0].set_xlabel('Quantis TeÃ³ricos')
        axes[1,0].set_ylabel('Quantis Observados')
        axes[1,0].set_title('Q-Q Plot (Normalidade)', fontweight='bold')
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. ResÃ­duos absolutos vs PrevisÃµes
        axes[1,1].scatter(predictions, np.abs(residuals), alpha=0.6, color='orange')
        axes[1,1].set_xlabel('PrevisÃµes')
        axes[1,1].set_ylabel('ResÃ­duos Absolutos')
        axes[1,1].set_title('ResÃ­duos Absolutos vs PrevisÃµes', fontweight='bold')
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def plot_cross_validation_results(self) -> None:
        """Plota resultados da validaÃ§Ã£o cruzada."""
        if 'cross_validation' not in self.model_metrics:
            raise ValueError("ValidaÃ§Ã£o cruzada nÃ£o disponÃ­vel. Treine o modelo com with_cross_validation=True.")
        
        cv_data = self.model_metrics['cross_validation']
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('ğŸ“Š VALIDAÃ‡ÃƒO CRUZADA - ESTABILIDADE DO MODELO', fontsize=16, fontweight='bold')
        
        # RÂ² scores
        axes[0].boxplot(cv_data['r2_scores'])
        axes[0].set_title(f'RÂ² Score\nMÃ©dia: {cv_data["r2_mean"]}')
        axes[0].set_ylabel('RÂ²')
        
        # RMSE scores
        axes[1].boxplot(cv_data['rmse_scores'])
        axes[1].set_title(f'RMSE\nMÃ©dia: {cv_data["rmse_mean"]}')
        axes[1].set_ylabel('RMSE')
        
        # MAE scores
        axes[2].boxplot(cv_data['mae_scores'])
        axes[2].set_title(f'MAE\nMÃ©dia: {cv_data["mae_mean"]}')
        axes[2].set_ylabel('MAE')
        
        plt.tight_layout()
        plt.show()
    
    def generate_report(self) -> str:
        """Gera relatÃ³rio completo."""
        if not self.model_metrics:
            return "Modelo nÃ£o treinado"
        
        report = ["="*70, "ğŸ† RELATÃ“RIO DO MODELO", "="*70]
        
        # MÃ©tricas bÃ¡sicas
        basic = self.model_metrics['basic_metrics']
        report.extend([
            "\nğŸ“Š PERFORMANCE:",
            f"   RÂ² (Teste): {basic['test_r2']:.4f} ({basic['test_r2']*100:.2f}%)",
            f"   RMSE: {basic['test_rmse']:.4f}",
            f"   MAE: {basic['test_mae']:.4f}"
        ])
        
        # ValidaÃ§Ã£o cruzada
        if 'cross_validation' in self.model_metrics:
            cv = self.model_metrics['cross_validation']
            report.extend([
                "\nğŸ”„ VALIDAÃ‡ÃƒO CRUZADA:",
                f"   RÂ² MÃ©dio: {cv['r2_mean']:.4f} Â±{cv['r2_std']:.4f}",
                f"   RMSE MÃ©dio: {cv['rmse_mean']:.4f} Â±{cv['rmse_std']:.4f}"
            ])
        
        # Robustez com ruÃ­do
        if 'noise_robustness' in self.model_metrics:
            report.append("\nğŸ›¡ï¸ ROBUSTEZ COM RUÃDO:")
            for noise_level, metrics in self.model_metrics['noise_robustness'].items():
                level = noise_level.split('_')[1]
                report.append(f"   RuÃ­do {level}: RÂ²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}")
        
        # ResÃ­duos
        if 'residual_analysis' in self.model_metrics:
            residual = self.model_metrics['residual_analysis']
            report.extend([
                "\nğŸ” RESÃDUOS:",
                f"   MÃ©dia: {residual['mean']:.4f}",
                f"   Desvio: {residual['std']:.4f}"
            ])
        
        report.append("="*70)
        return "\n".join(report)


# Exemplo de uso
if __name__ == "__main__":
    # DemonstraÃ§Ã£o da classe
    predictor = CropYieldPredictor(model_type='linear')
    
    # Carregando e treinando
    df = predictor.load_data('crop_yield_data.csv')
    X = df[predictor.feature_columns]
    y = df['crop_yield']
    
    # Treinamento robusto
    results = predictor.train(X, y, with_cross_validation=True, with_noise_test=True)
    
    # RelatÃ³rio
    print(predictor.generate_report())
    
    # PrevisÃ£o
    rendimento = predictor.predict_single(1500, 8, 500, 10, 1500)
    print(f"\nğŸ”® PrevisÃ£o: {rendimento:.2f} toneladas/hectare")
    
    # Salvando
    predictor.save_model('crop_yield_model.joblib')
    print("\nğŸ’¾ Modelo salvo!")